# -*- coding: utf-8 -*-
"""Phishing_url.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GW75bbAk3A6QMuTKYVnjNiD83TZZ4cT0
"""

import zipfile
with zipfile.ZipFile("/content/PhiUSIIL_Phishing_URL_Dataset.csv.zip", "r") as zip_ref:
    zip_ref.extractall("/content/")

!ls /content/

import pandas as pd
df = pd.read_csv("/content/PhiUSIIL_Phishing_URL_Dataset.csv")
print(df.shape)

df.info()

df.head(20)

df.describe()

df.isnull().sum()

"""THERE ARE NO MISSING VALUES"""

print("Duplicate rows:", df.duplicated().sum())

"""THERE ARE NO DUPLICATES

Less relevant or redundant columns were removed to reduce noise, prevent overfitting, and improve model performance. The remaining columns represent key URL-based, domain-based, and content-based features that are more likely to influence phishing detection.
"""

columns_to_remove = [
    # Page Content Features
    'LineOfCode', 'LargestLineLength', 'Title', 'NoOfImage', 'NoOfCSS', 'NoOfJS',
    'NoOfEmptyRef', 'NoOfSelfRef', 'NoOfExternalRef',

    # Weak Behavioral Indicators
    'HasSocialNet', 'HasCopyrightInfo', 'Robots', 'IsResponsive',

    # Highly Redundant / Derived
    'LetterRatioInURL', 'DegitRatioInURL', 'SpacialCharRatioInURL', 'ObfuscationRatio',

    # Rarely Helpful
    'CharContinuationRate', 'URLCharProb', 'URLSimilarityIndex',
    'TLDLength', 'DomainTitleMatchScore', 'URLTitleMatchScore',

    # HTML Metadata
    'HasDescription',

    # Form Details (overlap)
    'HasSubmitButton', 'HasHiddenFields'
]

# Drop only those columns that exist (avoid KeyError)
df.drop(columns=[c for c in columns_to_remove if c in df.columns], inplace=True)

print("✅ Columns removed.")
print("Remaining columns:", len(df.columns))

df.columns

df.dtypes

"""No further data type conversions are required.
All features are in appropriate formats for analysis and modeling.
"""

df.shape

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='label', data=df)
plt.title('Distribution of Target Variable (Legitimate vs Phishing)')
plt.xlabel('Label (1 = Legitimate, 0 = Phishing)')
plt.ylabel('Count')
plt.show()

# Count of each class
label_counts = df['label'].value_counts()

# Display counts
print("Label counts:")
print(label_counts)

# Calculate ratio and percentage
ratio = label_counts[1] / label_counts[0]
percentage = (label_counts / label_counts.sum()) * 100

print(f"\nRatio (Legitimate : Phishing) = {ratio:.2f} : 1")
print("\nPercentage distribution:")
print(percentage)

"""The target variable (label) is fairly balanced (≈57% legitimate, 43% phishing).

"""

plt.figure(figsize=(12, 8))
# Select only numerical columns before calculating correlation
numerical_df = df.select_dtypes(include=['number'])
sns.heatmap(numerical_df.corr(), cmap='coolwarm', annot=False)
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

numerical_cols = [
    'URLLength', 'DomainLength', 'NoOfSubDomain', 'NoOfObfuscatedChar',
    'NoOfLettersInURL', 'NoOfDegitsInURL', 'NoOfEqualsInURL',
    'NoOfQMarkInURL', 'NoOfAmpersandInURL', 'NoOfOtherSpecialCharsInURL',
    'NoOfURLRedirect', 'NoOfSelfRedirect', 'NoOfPopup', 'NoOfiFrame'
]

for col in numerical_cols:
    plt.figure(figsize=(6,4))
    sns.histplot(df[col], bins=30, kde=True)
    plt.title(f'Distribution of {col}')
    plt.show()

for col in numerical_cols:
    plt.figure(figsize=(6,4))
    sns.boxplot(x=df[col])
    plt.title(f'Boxplot of {col}')
    plt.show()

sns.boxplot(x='label', y='URLLength', data=df)
plt.title('URL Length vs Label')
plt.show()

sns.boxplot(x='label', y='NoOfSubDomain', data=df)
plt.title('No. of Subdomains vs Label')
plt.show()

binary_cols = [
    'IsDomainIP', 'HasObfuscation', 'IsHTTPS', 'HasTitle', 'HasFavicon',
    'HasExternalFormSubmit', 'HasPasswordField', 'Bank', 'Pay', 'Crypto'
]

for col in binary_cols:
    plt.figure(figsize=(5,3))
    sns.countplot(x=col, hue='label', data=df)
    plt.title(f'{col} vs Label (1=Legitimate, 0=Phishing)')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.legend(title='Label')
    plt.show()

"""OBSERVATIONS

1️⃣Distribution of Features

Most numerical features like URLLength, DomainLength, and NoOfSubDomain are right-skewed, indicating that a few URLs have exceptionally large values.

Many binary features (like IsDomainIP, IsHTTPS, HasTitle, HasFavicon, etc.) show values concentrated around 0 and 1 — as expected since these are boolean indicators.

The TLDLegitimateProb feature shows moderate variation, indicating differing legitimacy probabilities for domain extensions.

2️⃣ Outlier Detection (Box Plots)

Outliers are observed in features like URLLength, NoOfObfuscatedChar, and NoOfLettersInURL, possibly due to very long or heavily encoded URLs.

Since tree-based ensemble models (e.g., Random Forest, XGBoost) are not sensitive to outliers, they can be safely retained without scaling or transformation.

3️⃣ Correlation Analysis (Heatmap)

Most features show weak to moderate correlation, suggesting that each feature contributes unique information.

Some mild positive correlations exist between: URLLength and DomainLength,NoOfLettersInURL and NoOfSubDomain
The overall correlation pattern indicates low multicollinearity, which is ideal for ensemble models.

4️⃣ Target Variable Balance

The dataset is fairly balanced, with approximately 57% legitimate (label 1) and 43% phishing (label 0) URLs.

This balance ensures that models trained on this dataset will not suffer from significant bias toward one class.

✅ Overall Summary

No missing values or datatype issues.

Features are clean and well-prepared for modeling.


Dataset is large and balanced — suitable for reliable training and testing.


"""

from sklearn.preprocessing import LabelEncoder

# Copy dataset to avoid modifying original
df_encoded = df.copy()

# Columns to encode
cat_cols = ['Domain', 'TLD']

# Apply Label Encoding
le = LabelEncoder()
for col in cat_cols:
    df_encoded[col] = le.fit_transform(df_encoded[col])

# Drop the 'URL' column as it’s raw text and not suitable for direct encoding
df_encoded.drop(columns=['URL'], inplace=True)

df_encoded.head()

X = df_encoded.drop(columns=['label'])
y = df_encoded['label']

X.shape

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Training set shape:", X_train.shape)
print("Testing set shape:", X_test.shape)

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Initialize models
models = {
    "Random Forest": RandomForestClassifier(random_state=42),
    "AdaBoost": AdaBoostClassifier(random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
}

# Train and store results
results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    results[name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    }

print("✅ Model training completed.")

import pandas as pd

results_df = pd.DataFrame(results).T
print(results_df)

import matplotlib.pyplot as plt

results_df.plot(kind='bar', figsize=(10,6))
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.ylim(0,1)
plt.show()

"""OBSERVATIONS(Comparison of Model Metrics)

* All four ensemble models show very high performance (above 99%), indicating that the dataset is well-structured and the models have effectively captured important phishing patterns.



* XGBoost achieves the highest accuracy (99.88%) and best balance between precision, recall, and F1-score, making it the most reliable and generalizable model.


*   Random Forest performs very close to XGBoost, indicating strong robustness and low bias-variance trade-off.




*  Gradient Boosting also performs well but slightly below XGBoost in precision.


*  AdaBoost has slightly lower scores in all metrics, suggesting it may not capture complex feature interactions as effectively as the others.

Overall, XGBoost is the best-performing model — showing excellent predictive power, strong generalization, and consistent results across all evaluation metrics.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Predict on test data
y_pred_xgb = models['XGBoost'].predict(X_test)


# Compute metrics
accuracy = accuracy_score(y_test, y_pred_xgb)
precision = precision_score(y_test, y_pred_xgb)
recall = recall_score(y_test, y_pred_xgb)
f1 = f1_score(y_test, y_pred_xgb)

print("XGBoost Model Performance on Test Data")
print("--------------------------------------")
print(f"Accuracy : {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall   : {recall:.4f}")
print(f"F1 Score : {f1:.4f}")

import seaborn as sns
import matplotlib.pyplot as plt

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_xgb)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('XGBoost Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_xgb, target_names=['Phishing (0)', 'Legitimate (1)']))

"""These results indicate that the model generalizes extremely well to unseen data, showing no signs of underfitting or overfitting.
Thus, the XGBoost model is highly reliable and effective for phishing URL detection in this dataset.
"""

# Compare training and testing accuracy
train_pred_xgb = models['XGBoost'].predict(X_train)
train_acc = accuracy_score(y_train, train_pred_xgb)
test_acc = accuracy_score(y_test, y_pred_xgb)

print(f"Training Accuracy: {train_acc:.4f}")
print(f"Testing Accuracy : {test_acc:.4f}")

if abs(train_acc - test_acc) < 0.01:
    print("✅ Model generalizes well — no significant overfitting.")
else:
    print("⚠️ Possible overfitting detected. Consider cross-validation or regularization.")

from xgboost import plot_importance
import matplotlib.pyplot as plt

# Plot using XGBoost’s built-in method
plt.figure(figsize=(10, 8))
plot_importance(models['XGBoost'], max_num_features=15, importance_type='weight')
plt.title('Top 15 Important Features (XGBoost)')
plt.show()

"""Features such as Domain, DomainLength,NoOfLettersinURL and TLD were among the most influential in distinguishing between phishing and legitimate URLs.
These features capture key structural and statistical characteristics of URLs that strongly indicate phishing tendencies—such as unusually long URLs, suspicious domain patterns, or low TLD legitimacy probabilities.


"""

import joblib

# Save the best model (for example, XGBoost)
joblib.dump(models['XGBoost'], 'xgboost_phishing_model.pkl')

print("✅ Model saved successfully as 'xgboost_phishing_model.pkl'")

# Load the model
loaded_model = joblib.load('xgboost_phishing_model.pkl')

print("✅ Model loaded successfully!")

sample = X_test.iloc[:10]
preds = loaded_model.predict(sample)
preds